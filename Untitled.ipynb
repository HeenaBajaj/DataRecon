{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtWidgets, uic\n",
    "from PyQt5 import QtCore\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pyodbc  as odbc\n",
    "import datetime \n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3 as s\n",
    "app=QtWidgets.QApplication([])\n",
    "item=QtWidgets.QTableWidgetItem\n",
    "call=uic.loadUi(\"front.ui\")\n",
    "call=uic.loadUi(\"rw.ui\")\n",
    "########\n",
    "str1='actuarialdb'\n",
    "str2='actuarialdb'\n",
    "str3='actuarialdb'\n",
    "str4='actuarialdb'\n",
    "str5='dbo.[super actsubdb_miu_2020Q1_bookings_final]'\n",
    "str6='dbo.[MIU_SISAD_ITC_202001]'\n",
    "str7=''\n",
    "strfil = ''\n",
    "connectionstr2 ='a'\n",
    "connectionstr1 ='b'\n",
    "obu_names = \"'International - Casualty Primary'\"\n",
    "d = 1\n",
    "file_level = 1\n",
    "recon_type='ACS'\n",
    "save_file_path = r'C:\\Users\\x116135\\Desktop\\SISAD Q3 ITC\\Losses'\n",
    "\n",
    "#########\n",
    "connt = odbc.connect('Driver={SQL Server};'\n",
    "                      'Server=actuarialdb;'\n",
    "                      'Database=actuarialdb;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "call.typeofrecons.clear()\n",
    "recontypeqry=\"SELECT Distinct [Type_Of_recon] as Type_Of_recon  FROM dbo.rt_t1100_DataReconMappings\"\n",
    "df_rec_type= pd.read_sql(recontypeqry, connt)\n",
    "df_rec_type.reset_index(drop=True)\n",
    "call.typeofrecons.addItems(df_rec_type['Type_Of_recon'])\n",
    "\n",
    "nameqry=\"select Top 1 Name from dbo.actuarial_users where 'R02\\\\'+[emp id]=user\"\n",
    "username=pd.read_sql(nameqry, connt)\n",
    "username.reset_index(drop=True,inplace=True)\n",
    "username =username.iat[0,0]\n",
    "username=\"Welcome: \"+username\n",
    "call.label_15.setText(username)\n",
    "\n",
    "def withoutediting():\n",
    "    global recon_type\n",
    "    recon_type=call.typeofrecons.currentText()\n",
    "    print(recon_type)\n",
    "\n",
    "def withouteditingtable():\n",
    "    global call,app,connt\n",
    "    global df  \n",
    "    app.exec_()\n",
    "    # app=QtWidgets.QApplication([])\n",
    "    call=uic.loadUi(\"rw.ui\")\n",
    "    call.typeofrecons.clear()\n",
    "    recontypeqry=\"SELECT Distinct [Type_Of_recon] as Type_Of_recon  FROM dbo.rt_t1100_DataReconMappings\"\n",
    "    df_rec_type= pd.read_sql(recontypeqry, connt)\n",
    "    df_rec_type.reset_index(drop=True)\n",
    "    call.typeofrecons.addItems(df_rec_type['Type_Of_recon'])\n",
    "\n",
    "    nameqry=\"select Top 1 Name from dbo.actuarial_users where 'R02\\\\'+[emp id]=user\"\n",
    "    username=pd.read_sql(nameqry, connt)\n",
    "    username.reset_index(drop=True,inplace=True)\n",
    "    username =username.iat[0,0]\n",
    "    username=\"Welcome: \"+username\n",
    "    call.label_15.setText(username)\n",
    "\n",
    "\n",
    "    call.show()\n",
    "    app.exec()\n",
    "    call.pushButton2.clicked.connect(clearcombo)\n",
    "    call.saveserver.clicked.connect(getdb)\n",
    "    call.savedb.clicked.connect(gettables)\n",
    "    call.savetable.clicked.connect(getall)\n",
    "    call.pushButton.clicked.connect(runAll)\n",
    "    call.addfilter.clicked.connect(filteraddition)\n",
    "    call.addmeasure.clicked.connect(filteradditionmeasure)\n",
    "    call.confirmfilter.clicked.connect(filterfinal)\n",
    "    call.confirmmeasure.clicked.connect(filterfinalmeasure)\n",
    "    call.confirmfilter0.clicked.connect(setfiltercolumn)\n",
    "    call.editmappings.clicked.connect(table_ui)\n",
    "    call.woediting.clicked.connect(withoutediting)\n",
    "\n",
    "#######\n",
    "def table_ui():\n",
    "    global call,app\n",
    "    global recon_type,horHeaders,df\n",
    "    conn = odbc.connect('Driver={SQL Server};'\n",
    "                      'Server=actuarialdb;'\n",
    "                      'Database=actuarialdb;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "    \n",
    "    recon_type=call.typeofrecons.currentText()\n",
    "    print(recon_type)\n",
    "    Query = \"select [SNo], [List of Columns],[List of Columns1],[Measure/Dimension],[Filter],[NULL Check],[Distinct Check],[Granularity],[Priority],\"\n",
    "    Query = Query+\"[Length Check],[Datatype Check],[sign_convention],[type_of_measure] from [dbo].[rt_t1100_DataReconMappings] where [Type_Of_recon] = '\"+recon_type+\"'\"\n",
    "    df1= pd.read_sql(Query, conn)\n",
    "    t1=df1.iat[0,0]\n",
    "    df=df1.copy()\n",
    "    df.fillna('',inplace=True)\n",
    "\n",
    "    app.exec_()\n",
    "    # app=QtWidgets.QApplication([])\n",
    "    call=uic.loadUi(\"front.ui\")\n",
    "    call.show()\n",
    "    app.exec()\n",
    "\n",
    "    call.Table1.setRowCount(df.shape[0])\n",
    "    call.Table1.setColumnCount(df.shape[1])\n",
    "    horHeaders=df1.columns\n",
    "    call.Table1.setHorizontalHeaderLabels(horHeaders)\n",
    "    call.Table1.setHorizontalHeaderLabels(horHeaders)\n",
    "\n",
    "    for row in range (0, df.shape[0]):\n",
    "        for col in range (0, df.shape[1]):\n",
    "            data = str(df.iat[row, col] )\n",
    "            call.Table1.setItem(row, col, item(str(data)))\n",
    "\n",
    "    call.pushButton.clicked.connect(rw_ui)\n",
    "    call.woeditingTable.clicked.connect(withouteditingtable)\n",
    "    call.rowadd.clicked.connect(addrow)\n",
    "    # app.exec_()\n",
    "    # app=QtWidgets.QApplication([])\n",
    "    # call=uic.loadUi(\"rw.ui\")\n",
    "    # call.show()\n",
    "    # app.exec()\n",
    "\n",
    "\n",
    "def getdb():\n",
    "    global str1\n",
    "    global str2\n",
    "    \n",
    "    str1=call.s1.currentText()\n",
    "    connectionstr1='Driver={SQL Server}; Server='+str1.strip()+'; Database=master; Trusted_Connection=yes;'\n",
    "    connection1 = odbc.connect(connectionstr1)\n",
    "    sqldb=\"SELECT DB_NAME(database_id) AS DatabaseName FROM sys.databases\"\n",
    "    df1= pd.read_sql(sqldb, connection1)\n",
    "    df1.reset_index(drop=True)\n",
    "    call.db1.clear()\n",
    "    call.db1.addItems(df1['DatabaseName'])\n",
    "    str2=call.s2.currentText()\n",
    "    connectionstr2='Driver={SQL Server}; Server='+str2.strip()+'; Database=master; Trusted_Connection=yes;'\n",
    "    connection2 = odbc.connect(connectionstr2)\n",
    "    sqldb=\"SELECT DB_NAME(database_id) AS DatabaseName FROM sys.databases\"\n",
    "    df2= pd.read_sql(sqldb, connection2)\n",
    "    df2.reset_index(drop=True)\n",
    "    call.db2.clear()\n",
    "    call.db2.addItems(df2['DatabaseName'])\n",
    "\n",
    "def gettables():   \n",
    "    global str3\n",
    "    global str4\n",
    "    global connectionstr1\n",
    "    global connectionstr2\n",
    "    call.t1.clear()\n",
    "    call.t2.clear()\n",
    "    str3=call.db1.currentText()\n",
    "    dbqry1=\"SELECT TABLE_SCHEMA+'.['+TABLE_NAME+']' as TableName FROM INFORMATION_SCHEMA.TABLES where (TABLE_NAME like '%super%actsubdb_miu%' or TABLE_NAME like '%tisad%') order by 1\"\n",
    "    connectionstr1='Driver={SQL Server}; Server='+str1+'; Database='+str3+'; Trusted_Connection=yes;'\n",
    "    connection1 = odbc.connect(connectionstr1)    \n",
    "    dff1= pd.read_sql(dbqry1, connection1)\n",
    "    dff1.reset_index(drop=True)\n",
    "    call.t1.addItems(dff1['TableName'])\n",
    "    str4=call.db2.currentText()\n",
    "    dbqry2=\"SELECT TABLE_SCHEMA+'.['+TABLE_NAME+']' as TableName FROM INFORMATION_SCHEMA.TABLES where TABLE_NAME like '%sisad%' order by 1\"\n",
    "    connectionstr2='Driver={SQL Server}; Server='+str2+'; Database='+str4+'; Trusted_Connection=yes;'\n",
    "    connection2 = odbc.connect(connectionstr2)    \n",
    "    dff2= pd.read_sql(dbqry2, connection2)\n",
    "    dff2.reset_index(drop=True)\n",
    "    call.t2.addItems(dff2['TableName'])\n",
    "\n",
    "def getall():   \n",
    "    global str5\n",
    "    global str6\n",
    "    global str7\n",
    "    global obu_names\n",
    "    global df1,recon_type\n",
    "    str5=call.t1.currentText()\n",
    "    str6=call.t2.currentText()\n",
    "    call.filter_down.clear()\n",
    "    # dbqry5=\"SELECT distinct [OperatingUnit/OBU] as obu FROM \"+str6    \n",
    "    # connection2 = odbc.connect(connectionstr2)    \n",
    "    # obudf= pd.read_sql(dbqry5, connection2)\n",
    "    # df0.reset_index(drop=True)\n",
    "    conn = odbc.connect('Driver={SQL Server};'\n",
    "                      'Server=actuarialdb;'\n",
    "                      'Database=actuarialdb;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "    Query = \"select [SNo], [List of Columns],[List of Columns1],[Measure/Dimension],[Filter],[NULL Check],[Distinct Check],[Granularity],[Priority],\"\n",
    "    Query = Query+\"[Length Check],[Datatype Check],[sign_convention],[type_of_measure] from [dbo].[rt_t1100_DataReconMappings] where [Type_Of_recon] = '\"+recon_type+\"'\"\n",
    "    df1= pd.read_sql(Query, conn)\n",
    "    print(df1)\n",
    "    \n",
    "    call.filter_down.addItems(df1['List of Columns'])\n",
    "    # call.obuset.setText(\"\")\n",
    "    str7=\"\"\n",
    "    # print(str5)\n",
    "    # print(str6)\n",
    "\n",
    "def setfiltercolumn():\n",
    "    global df1,connectionstr1,connectionstr2,d,file_level,recon_type,connt,flist\n",
    "    connection1=odbc.connect(connectionstr1)\n",
    "    connection2=odbc.connect(connectionstr2)\n",
    "    strs=call.filter_down.currentText()\n",
    "    call.filterbox.clear()\n",
    "    print(df1)\n",
    "    d=df1[df1['List of Columns']==strs].SNo.item()\n",
    "    Sr_filelevel = d\n",
    "    file_level = Sr_filelevel\n",
    "    d1=df1[df1['SNo']==d]['List of Columns'].item()\n",
    "    d2=df1[df1['SNo']==d]['List of Columns1'].item()\n",
    "    filterqry1=\"SELECT distinct \"+d1+\"as a from \"+str5\n",
    "    filterqry2=\"SELECT distinct \"+d2+\"as a from \"+str6\n",
    "    f1=pd.read_sql(filterqry1, connection1)\n",
    "    f2=pd.read_sql(filterqry2, connection2)\n",
    "    flist = pd.concat([f1, f2], axis=0, sort=False)\n",
    "    flist.drop_duplicates(inplace=True)\n",
    "    flist.reset_index(drop=True,inplace=True)\n",
    "    filtermeasure1 = \"SELECT distinct type_of_measure as b from dbo.[rt_t1100_DataReconMappings] ct WHERE Type_of_Recon = '%s' and type_of_measure is not  null\" %(recon_type)\n",
    "    f3 = pd.read_sql(filtermeasure1, connt)\n",
    "    f3.reset_index(drop=True,inplace=True)\n",
    "    print(f3)\n",
    "    call.filtermeasure.clear()\n",
    "    call.filterbox.addItems(['All'])\n",
    "    call.filterbox.addItems(flist['a'])\n",
    "    print('about to fill measure')\n",
    "    call.filtermeasure.addItems(f3['b'])\n",
    "\n",
    "def filteraddition():\n",
    "    global str7\n",
    "    str7=str7+\"'\"+call.filterbox.currentText()+\"',\"\n",
    "    call.filterf.setText(str7)\n",
    "\n",
    "def filteradditionmeasure():\n",
    "    global strfil\n",
    "    strfil=strfil+\"'\"+call.filtermeasure.currentText()+\"',\"\n",
    "    call.checkboxmeasure.setText(strfil)\n",
    "\n",
    "def filterfinal():\n",
    "    global obu_names,flist\n",
    "    obu_names=str7[:-1]\n",
    "\n",
    "    if obu_names == \"'All'\":\n",
    "        flist['a'] = \"'\" + flist['a'] + \"'\"\n",
    "        strflist = \",\".join(flist['a'])\n",
    "        obu_names=strflist[:]\n",
    "        print(obu_names)\n",
    "    elif obu_names != \"'All'\":\n",
    "        obu_names=''+obu_names+''\n",
    "    \n",
    "    obu_names=obu_names.replace(\"'\",\"''\")\n",
    "    print(obu_names)\n",
    "\n",
    "def filterfinalmeasure():\n",
    "    global measure_names,strfil\n",
    "    measure_names=strfil[:-1]\n",
    "    measure_names=''+measure_names+''\n",
    "    # measure_names=measure_names.replace(\"'\",\"''\")\n",
    "    print(measure_names)\n",
    "\n",
    "def clearcombo():\n",
    "    call.db1.clear()\n",
    "    call.db2.clear()\n",
    "    call.t1.clear()\n",
    "    call.t2.clear()\n",
    "    call.filterbox.clear()\n",
    "\n",
    "def dtcheck():\n",
    "    global df0\n",
    "    #mapping table to be fetched from Connection 1\n",
    "    # connectionstr1\n",
    "    connection1 = odbc.connect(connectionstr1)    \n",
    "    mapping_query = \"select * from [dbo].[rt_t1100_DataReconMappings] where \"\n",
    "    df0= pd.read_sql(mapping_query, connection1)\n",
    "    df1=df0[df0['Distinct Check']==1]\n",
    "    columns1 = df1['List of Columns'].copy()\n",
    "    columns2 = df1['List of Columns1'].copy()\n",
    "    columns1=columns1.str.replace(\"[\",\"'\")\n",
    "    columns1=columns1.str.replace(\"]\",\"'\")\n",
    "    columns2=columns2.str.replace(\"[\",\"'\")\n",
    "    columns2=columns2.str.replace(\"]\",\"'\")\n",
    "    column1 = columns1.str.cat(sep=',')\n",
    "    column2 = columns2.str.cat(sep=',')\n",
    "    column1=\"(\"+column1+\")\"\n",
    "    column2=\"(\"+column2+\")\"\n",
    "\n",
    "    # s1=call.server1.toPlainText()\n",
    "    # s2=call.server2.toPlainText()\n",
    "    # d1=call.database1.toPlainText()\n",
    "    # d2=call.database2.toPlainText()\n",
    "    # t1=call.tablename1.toPlainText()\n",
    "    # t2=call.tablename2.toPlainText()\n",
    "    # connectionstr1='Driver={SQL Server}; Server='+s1.strip()+'; Database='+d1.strip()+'; Trusted_Connection=yes;'\n",
    "    # connectionstr2='Driver={SQL Server}; Server='+s2.strip()+'; Database='+d2.strip()+'; Trusted_Connection=yes;'\n",
    "    connection1 = odbc.connect(connectionstr1)\n",
    "    connection2 = odbc.connect(connectionstr2)\n",
    "\n",
    "\n",
    "    check1=\"SELECT distinct upper(c.name) 'Column Name', t.Name 'Data type', c.max_length 'Max Length' \"\n",
    "    check1=check1+\"FROM sys.columns c \"\n",
    "    check1=check1+\"INNER JOIN \"\n",
    "    check1=check1+\"    sys.types t ON c.user_type_id = t.user_type_id \"\n",
    "    check1=check1+\"LEFT OUTER JOIN \"\n",
    "    check1=check1+\"    sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id \"\n",
    "    check1=check1+\" LEFT OUTER JOIN \"\n",
    "    check1=check1+\"    sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id \"\n",
    "    check1=check1+\" WHERE \"\n",
    "    check1=check1+\"    c.object_id = OBJECT_ID('\"+str5+\"')\"\n",
    "    check1=check1+\"and c.name in %s\" %(column1)\n",
    "\n",
    "    check2=\"SELECT distinct upper(c.name) 'Column Name', t.Name 'Data type', c.max_length 'Max Length' \"\n",
    "    check2=check2+\"FROM sys.columns c \"\n",
    "    check2=check2+\"INNER JOIN \"\n",
    "    check2=check2+\"    sys.types t ON c.user_type_id = t.user_type_id \"\n",
    "    check2=check2+\"LEFT OUTER JOIN \"\n",
    "    check2=check2+\"    sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id \"\n",
    "    check2=check2+\" LEFT OUTER JOIN \"\n",
    "    check2=check2+\"    sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id \"\n",
    "    check2=check2+\" WHERE \"\n",
    "    check2=check2+\"    c.object_id = OBJECT_ID('\"+str6+\"')\"\n",
    "    check2=check2+\"and c.name in %s\" %(column2)\n",
    "   \n",
    "    datatype1= pd.read_sql(check1, connection1)\n",
    "    datatype2= pd.read_sql(check2, connection2)\n",
    "\n",
    "    dfjoin=datatype1.merge(datatype2,how='outer' ,left_on='Column Name', right_on='Column Name')\n",
    "    # dfjoin = pd.merge(datatype1,datatype2,on=['Column Name'])\n",
    "    dfjoin['Datatype Check'] = 'other'\n",
    "    dfjoin.loc[dfjoin['Data type_x'] == dfjoin['Data type_y'], 'Datatype Check'] = 'match'\n",
    "    dfjoin.loc[dfjoin['Data type_x'] != dfjoin['Data type_y'], 'Datatype Check'] = 'mismatch'\n",
    "\n",
    "    dfjoin['Datatype Length Check'] = 'other'\n",
    "    dfjoin.loc[dfjoin['Max Length_x'] == dfjoin['Max Length_y'], 'Datatype Length Check'] = 'match'\n",
    "    dfjoin.loc[dfjoin['Max Length_x'] != dfjoin['Max Length_y'], 'Datatype Length Check'] = 'mismatch'\n",
    "\n",
    "    dfjoin.rename(columns={\"Max Length_x\": \"Max Length table1\", \"Max Length_y\": \"Max Length table2\"},inplace=True)\n",
    "    dfjoin.rename(columns={\"Data type_x\": \"Data type table1\", \"Data type_y\": \"Data type table2\"},inplace=True)\n",
    "\n",
    "    dfjoin_final=dfjoin.copy()\n",
    "    dfjoin_final=dfjoin_final.fillna('NA')\n",
    "    filter1 = dfjoin_final['Datatype Check']=='mismatch'\n",
    "    filter2 = dfjoin_final['Datatype Length Check']=='mismatch'\n",
    "    # print(filter1)\n",
    "    # filter1= filter1.fillna('dummy')\n",
    "    # print(filter2)\n",
    "    # filter2= filter1.fillna('dummy')\n",
    "    dfjoin_final.where(filter1 | filter2, inplace = True)\n",
    "    dfjoin_final.dropna(inplace=True)\n",
    "    # print(dfjoin)\n",
    "    print(dfjoin_final)\n",
    "\n",
    "    f_path = '%s\\dfjoin_final.xlsx' %(save_file_path)\n",
    "    dfjoin_final.to_excel(f_path,sheet_name = \"Datatype_Check\",index=False) \n",
    "    # dfjoin.to_excel(r'C:\\Users\\X133989\\Desktop\\python\\dfjoin.xlsx',sheet_name = \"Datatype_Check\",index=False) \n",
    "    print(\"Datatype Check is Complete.\")\n",
    "\n",
    "def DebugDifferences(GrainChecks_qry_Res,priorities,priorities1,measures_2):\n",
    "    data = pd.DataFrame([])\n",
    "    data3= pd.DataFrame([])\n",
    "    for i in range(len(priorities)):\n",
    "        df = priorities['List of Columns'].iloc[i]\n",
    "        grains = [df]\n",
    "        grains = pd.Series(grains).append(measures_2)\n",
    "        grains = grains.apply(lambda x: x[1:len(x)-1])\n",
    "        df_debug = GrainChecks_qry_Res[grains]\n",
    "        df_debug = df_debug.groupby(df[1:len(df)-1]).sum()\n",
    "        df_debug = df_debug.round(0)\n",
    "        data2 = pd.DataFrame(columns = ['filter','column'])\n",
    "        data4 = pd.DataFrame(columns = ['filter'])\n",
    "        for index, row in df_debug.iterrows():\n",
    "            df1 = row.abs()\n",
    "            check = df1.sum()\n",
    "            if check != 0:\n",
    "                data1 = pd.DataFrame(columns = ['filter'])\n",
    "                dat = pd.DataFrame(columns = ['filter'])\n",
    "                index1 = \"'\"  + index + \"'\"\n",
    "                data1['filter'] = [index1]\n",
    "                dat['filter'] = [index]\n",
    "                data4 = data4.append(dat)\n",
    "                data2 = data2.append(data1)\n",
    "                \n",
    "        if data2['filter'].empty:\n",
    "            data['filter'] = [str(-999)]\n",
    "        else:\n",
    "            data['filter'] = [str((data2['filter']).str.cat(sep=','))]\n",
    "        data['column'] = [df]\n",
    "        data['column1'] = [priorities1['List of Columns1'].iloc[i]]\n",
    "        GrainChecks_qry_Res = GrainChecks_qry_Res[GrainChecks_qry_Res[df[1:len(df)-1]].isin(data4['filter'])]\n",
    "        data3 = data3.append(data)\n",
    "    return data3\n",
    "\n",
    "def GetPriorities(dimensions,measures):\n",
    "    priorities = measures.loc[measures['Priority'].notnull(),['type_of_measure','Priority']]\n",
    "    priorities = priorities.groupby('type_of_measure').head(1).reset_index(drop=True)\n",
    "    priorities_columns = pd.DataFrame([])\n",
    "    for index, row in priorities.iterrows():\n",
    "        S1 = row['Priority'] \n",
    "        S1 = S1.split(\",\") \n",
    "        S1 = [int(i) for i in S1] \n",
    "        df=dimensions.set_index('SNo').loc[S1].reset_index()\n",
    "        priorities_columns1 = pd.DataFrame([])\n",
    "        priorities_columns1['List of Columns'] = df['List of Columns']\n",
    "        priorities_columns1['List of Columns1'] = df['List of Columns1']\n",
    "        priorities_columns1['type_of_measure'] = row['type_of_measure']\n",
    "        priorities_columns = priorities_columns.append(priorities_columns1)\n",
    "    return priorities_columns\n",
    "\n",
    "def debugCheck():\n",
    "    global str5,str6,obu_names,file_level,d\n",
    "    conn = odbc.connect(connectionstr1)    \n",
    "    conn1 = odbc.connect(connectionstr2)\n",
    "    Start_Time = datetime.datetime.now()\n",
    "\n",
    "    Query = \"Update ct set ct.Filter = 'IN ( %s )' FROM dbo.[rt_t1100_DataReconMappings] ct WHERE SNo = '%s' and Type_of_Recon = '%s' and isnull(type_of_measure,'') in ('',%s)\" % (obu_names, file_level,recon_type,measure_names)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(Query)\n",
    "\n",
    "    Query = \"select * from [dbo].[rt_t1100_DataReconMappings] where Type_of_Recon = '%s' and isnull(type_of_measure,'') in ('',%s)\" %(recon_type,measure_names)\n",
    "\n",
    "    df1= pd.read_sql(Query, conn)\n",
    "\n",
    "    dimensions = df1[df1['Measure/Dimension']=='D']\n",
    "    measures = df1[df1['Measure/Dimension']=='M']\n",
    "    File_Name_Level_1 = dimensions.loc[dimensions['SNo']== file_level,'List of Columns']\n",
    "    File_Name_Level_2 = dimensions.loc[dimensions['SNo']== file_level,'List of Columns1']\n",
    "\n",
    "    column1 = File_Name_Level_1.apply(lambda x: x[1:len(x)-1])\n",
    "    column2 = File_Name_Level_2.apply(lambda x: x[1:len(x)-1])\n",
    "\n",
    "    #getpropritylist for Premiums\n",
    "    priorities_columns = GetPriorities(dimensions,measures)\n",
    "\n",
    "    type_of_measures = pd.Series(measures['type_of_measure']).unique()\n",
    "\n",
    "    for k in range(len(type_of_measures)):\n",
    "        priorities = priorities_columns.loc[priorities_columns['type_of_measure'] == type_of_measures[k],['List of Columns']]\n",
    "        priorities1 = priorities_columns.loc[priorities_columns['type_of_measure'] == type_of_measures[k],['List of Columns1']]\n",
    "        \n",
    "       \n",
    "        #CREATING FILTERS FOR TABLE 1\n",
    "        dimensions_1 = priorities_columns.loc[priorities_columns['type_of_measure'] == type_of_measures[k],['List of Columns']]\n",
    "        dimensions_1 = pd.Series(dimensions_1.iloc[:,0])\n",
    "        measures_1 = measures['List of Columns']\n",
    "        #CREATING ISNULL(FILTER COLUMNS)\n",
    "        dimensions_filter = dimensions.copy()\n",
    "        dimensions_filter['List of Columns'] = \"isnull(\" + dimensions_filter['List of Columns'] + \",'')\"\n",
    "        filters_1 = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns','Filter']]\n",
    "        filters_1 = filters_1.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "\n",
    "        #CREATING FILTERS FOR TABLE 2\n",
    "        dimensions_2 = priorities_columns.loc[priorities_columns['type_of_measure'] == type_of_measures[k],['List of Columns1']]\n",
    "        dimensions_2 = pd.Series(dimensions_2.iloc[:,0])\n",
    "        measures_2 = measures['List of Columns1']\n",
    "\n",
    "        #CREATING ISNULL(FILTER COLUMNS)\n",
    "        dimensions_filter['List of Columns1'] = \"isnull(\" + dimensions_filter['List of Columns1'] + \",'')\"\n",
    "        filters_2 = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns1','Filter']]\n",
    "        filters_2 = filters_2.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "        #CREATING SELECT GRAINS FROM TABLE 1\n",
    "        Grains_Select1 = \"upper(ltrim(rtrim(\" + dimensions_1  + \"))) \"  + dimensions_1\n",
    "        Grains_Select1 = Grains_Select1.str.cat(sep=',') \n",
    "        \n",
    "        #CREATING SELECT GRAINS FROM TABLE 2\n",
    "        Grains_Select2 = \"upper(ltrim(rtrim(\" + dimensions_2 + \"))) \"  + dimensions_1\n",
    "        Grains_Select2 = Grains_Select2.str.cat(sep=',') \n",
    "\n",
    "        #CREATING GROUP BY GRAINS FROM TABLE 1\n",
    "        dimensions_1 = \"upper(ltrim(rtrim(\" + dimensions_1  + \"))) \"\n",
    "        Grains_1 = dimensions_1.str.cat(sep=',') \n",
    "    \n",
    "        #CREATING GROUP BY GRAINS FROM TABLE 2\n",
    "        dimensions_2 = \"upper(ltrim(rtrim(\" + dimensions_2  + \"))) \"\n",
    "        Grains_2 = dimensions_2.str.cat(sep=',') \n",
    "\n",
    "        measures_G = measures[measures['type_of_measure']==type_of_measures[k]]\n",
    "        #CREATING PREMIUM MEASURES \n",
    "        measures_1 = measures_G['List of Columns']\n",
    "        measures_2 = measures_G['List of Columns1']\n",
    "\n",
    "        #TABLE1\n",
    "        measures_grain_check_1 = \"Sum(\" + measures_1 + \") \" + measures_2\n",
    "        measures_grain_check_1 = measures_grain_check_1.str.cat(sep=',') \n",
    "\n",
    "        #TABLE2\n",
    "        measures_grain_check_2 = measures_G['sign_convention'] + \"*-1*Sum(\" + measures_G['List of Columns1'] + \")\" + \" \" + + measures_2\n",
    "        measures_grain_check_2 = measures_grain_check_2.str.cat(sep=',')\n",
    "\n",
    "        #HAVING CONDITION\n",
    "        measures_grain_check_H = \" Sum(\" + measures_1 + \") <> 0 \" \n",
    "        measures_grain_check_H = measures_grain_check_H.str.cat(sep='OR') \n",
    "        \n",
    "        measures_grain_check_H_2 = \" Sum(\" + measures_2 + \") <> 0 \" \n",
    "        measures_grain_check_H_2 = measures_grain_check_H_2.str.cat(sep='OR') \n",
    "    \n",
    "        GrainChecks_qry1 = \"\"\"SELECT 'Actuarialdb' source,  %s ,\n",
    "                    %s \n",
    "                    FROM  %s c \n",
    "                    WHERE (%s )\n",
    "                    GROUP BY %s\n",
    "                    HAVING (%s)\n",
    "                    \"\"\" % (Grains_Select1, measures_grain_check_1,str5,filters_1, Grains_1,measures_grain_check_H)\n",
    "            \n",
    "        GrainChecks_qry2 = \"\"\"SELECT  'SISAD' source , %s ,\n",
    "                        %s \n",
    "                        FROM  %s\n",
    "                        WHERE ( %s )\n",
    "                        GROUP BY %s\n",
    "                        HAVING (%s)\n",
    "                    \"\"\" % (Grains_Select2, measures_grain_check_2,str6,filters_2, Grains_2,measures_grain_check_H_2)\n",
    "        GrainChecks_qry_Res1 = pd.read_sql(GrainChecks_qry1, conn)\n",
    "        GrainChecks_qry_Res2 = pd.read_sql(GrainChecks_qry2, conn1)\n",
    "        GrainChecks_qry_Res=  pd.concat([GrainChecks_qry_Res1, GrainChecks_qry_Res2])\n",
    "\n",
    "        d= pd.Series((GrainChecks_qry_Res[File_Name_Level_1.apply(lambda x: x[1:len(x)-1])].iloc[:,0])).unique()\n",
    "       \n",
    "        OBU = ''.join(e for e in obu_names[1:len(obu_names)-1] if e.isalnum())\n",
    "        \n",
    "        from pandas import ExcelWriter\n",
    "        from openpyxl import load_workbook\n",
    "        \n",
    "\n",
    "        for i in range(len(d)):\n",
    "            if k == 0:\n",
    "                FilePath = r\"%s\\Debug\\%s.xlsx\" %(save_file_path,'D-'+d[i]) \n",
    "                writer = pd.ExcelWriter(FilePath)\n",
    "                print(\"Created new file\")\n",
    "            else:\n",
    "                print(\"Reopened file\")\n",
    "                FilePath = r\"%s\\Debug\\%s.xlsx\" %(save_file_path,'D-'+d[i]) \n",
    "                book = load_workbook(FilePath)\n",
    "                writer = pd.ExcelWriter(FilePath, engine='openpyxl') \n",
    "                writer.book = book\n",
    "                writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "            e=GrainChecks_qry_Res[column1] == d[i]\n",
    "            es = pd.Series(e.iloc[:,0])\n",
    "            Debug_check = GrainChecks_qry_Res[es]\n",
    "            sheetname = r'%s Check on Debug Fields'%(type_of_measures[k])\n",
    "            Debug_check.to_excel(writer, sheet_name=sheetname)\n",
    "                \n",
    "            comment = \"Debugging Recons for %s for:\"%(type_of_measures[k])\n",
    "            print(comment)\n",
    "            print(d[i])\n",
    "\n",
    "            data = DebugDifferences(Debug_check,priorities,priorities1,measures_2)\n",
    "            sheetname = '%s Debug Fields'%(type_of_measures[k])\n",
    "\n",
    "            data.to_excel(writer, sheet_name=sheetname)\n",
    "            indexNames = data[data['filter'] == '-999'].index      \n",
    "            data.drop(indexNames , inplace = True)\n",
    "\n",
    "            if data.empty==0:\n",
    "                data1 = \"isnull(\" + data['column'] +  \",'')\" + \" in ( \" + data['filter'] + \")\"\n",
    "                FilterstoDebug = data1.str.cat(sep=' AND ')\n",
    "                data12= \"isnull(\" + data['column1'] +  \",'')\" + \" in ( \" + data['filter'] + \")\"\n",
    "                FilterstoDebug1 = data12.str.cat(sep=' AND ')\n",
    "                \n",
    "                dimensions_filter = dimensions[~dimensions['List of Columns'].isin(File_Name_Level_1)]\n",
    "                dimensions_filter['List of Columns'] = \"isnull(\" + dimensions_filter['List of Columns'] + \",'')\"\n",
    "                filters_1_D = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns','Filter']]\n",
    "                if filters_1_D.empty==0:\n",
    "                    filters_1_D = filters_1_D.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "                    filters_1_debug = filters_1_D + \" AND \" + FilterstoDebug\n",
    "                else:\n",
    "                    filters_1_debug = FilterstoDebug\n",
    "\n",
    "                dimensions_filter['List of Columns1'] = \"isnull(\" + dimensions_filter['List of Columns1'] + \",'')\"\n",
    "                filters_2_D = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns1','Filter']]\n",
    "\n",
    "                if filters_2_D.empty==0:\n",
    "                    filters_2_D = filters_2_D.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "                    filters_2_debug = filters_2_D + \" AND \" + FilterstoDebug1\n",
    "                else:\n",
    "                    filters_2_debug = FilterstoDebug1\n",
    "\n",
    "\n",
    "                GrainChecks_qry1 = \"\"\"SELECT 'Actuarialdb' source,%s ,\n",
    "                            %s \n",
    "                            FROM  %s c \n",
    "                            WHERE ( %s )\n",
    "                            GROUP BY %s\n",
    "                            \"\"\" % (Grains_Select1, measures_grain_check_1,str5,filters_1_debug, Grains_1)\n",
    "                GrainChecks_qry2 = \"\"\"SELECT 'SiSAD' source, %s ,\n",
    "                                %s \n",
    "                                FROM  %s\n",
    "                                WHERE ( %s )\n",
    "                            GROUP BY %s\n",
    "                        \"\"\" % (Grains_Select2, measures_grain_check_2,str6,filters_2_debug, Grains_2)  \n",
    "        \n",
    "                GrainChecks_qry_Res1 = pd.read_sql(GrainChecks_qry1, conn)\n",
    "                GrainChecks_qry_Res2 = pd.read_sql(GrainChecks_qry2, conn1)\n",
    "\n",
    "                if len(GrainChecks_qry_Res1) == 0:\n",
    "                    GrainChecks_qry_Res_D = GrainChecks_qry_Res2\n",
    "                if len(GrainChecks_qry_Res2) == 0:\n",
    "                    GrainChecks_qry_Res_D = GrainChecks_qry_Res1\n",
    "                else:\n",
    "                    GrainChecks_qry_Res_D=  pd.concat([GrainChecks_qry_Res1, GrainChecks_qry_Res2])\n",
    "            else:\n",
    "                GrainChecks_qry_Res_D = pd.DataFrame([])\n",
    "            sheetname = \"%s Debugged\"%(type_of_measures[k])\n",
    "            GrainChecks_qry_Res_D.to_excel(writer, sheet_name=sheetname)\n",
    "            writer.save()\n",
    "            print(FilePath)\n",
    "            print(\"Saving the file\")\n",
    "    End_Time = datetime.datetime.now()\n",
    "    print(\"Debug Check is complete in: \") \n",
    "    print(End_Time-Start_Time)\n",
    "    print(\"Debugging is complete.\")  \n",
    "\n",
    "def nullcheck():\n",
    "\n",
    "    conn = odbc.connect(connectionstr1)  \n",
    "    conn1 = odbc.connect(connectionstr2)      \n",
    "    Query = \"SELECT * FROM [dbo].[rt_t1100_DataReconMappings]\"                       \n",
    "    df1= pd.read_sql(Query, conn)\n",
    "    dimension_1 = df1[df1['NULL Check']==1]\n",
    "    dimension_2 = dimension_1['List of Columns']\n",
    "    dimension_3 = dimension_1['List of Columns1']\n",
    "    writer = pd.ExcelWriter(r'%s\\CheckNull.xlsx') %(save_file_path)\n",
    "\n",
    "    for i in range(len(dimension_2)) : \n",
    "        qry_1 = \"\"\"SELECT *\n",
    "            FROM  (Select top 100 a.*, b.Bu_code from dbo.[super actsubdb_miu_2020Q1_bookings] a\n",
    "            left join sparta.dbo.rt_t0025_mas_le b\n",
    "                on a.[MIU Carco] = b.[MIU Carco]) c \n",
    "            WHERE %s IS NULL\n",
    "                \"\"\" % (dimension_2[i])\n",
    "        qry_2 = \"\"\"SELECT count(*)\n",
    "            FROM  (Select top 100 a.*, b.Bu_code from dbo.[super actsubdb_miu_2020Q1_bookings] a\n",
    "            left join sparta.dbo.rt_t0025_mas_le b\n",
    "                on a.[MIU Carco] = b.[MIU Carco]) c \n",
    "            WHERE %s IS NULL\n",
    "                \"\"\" % (dimension_2[i])  \n",
    "        GrainChecks_qry_Res_1= pd.read_sql(qry_2, conn).all()              \n",
    "        if(GrainChecks_qry_Res_1.all()!=0) :\n",
    "            GrainChecks_qry_Res= pd.read_sql(qry_1, conn)\n",
    "            sheet_name = \"A-\" + ''.join(e for e in dimension_2[i][1:len(dimension_2[i])-1] if e.isalnum())\n",
    "            GrainChecks_qry_Res.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    for j in range(len(dimension_3)) : \n",
    "        qry_2 = \"\"\" Select top 100 a.* from dbo.[MIU_SISAD_ITC_202001] a\n",
    "            WHERE %s IS NULL\n",
    "                \"\"\" % (dimension_3[j])\n",
    "        qry_3 = \"\"\" Select count(*) from dbo.[MIU_SISAD_ITC_202001] \n",
    "            WHERE %s IS NULL\n",
    "                \"\"\" % (dimension_3[j])                      \n",
    "        GrainChecks_qry_Res_3= pd.read_sql(qry_3, conn1).all()\n",
    "        if(GrainChecks_qry_Res_3.all()!=0) :\n",
    "            GrainChecks_qry_Res= pd.read_sql(qry_2, conn1)    \n",
    "            sheet_name = \"B-\" + ''.join(e for e in dimension_3[j][1:len(dimension_3[j])-1] if e.isalnum())\n",
    "            GrainChecks_qry_Res.to_excel(writer, sheet_name=sheet_name) \n",
    "        else :\n",
    "            print('no')\n",
    "\n",
    "    writer.save()  \n",
    "    print(\"Null Check is complete.\") \n",
    "\n",
    "def distinctCheck():\n",
    "    conn = odbc.connect(connectionstr1)\n",
    "    conn1 = odbc.connect(connectionstr2)\n",
    "    Query = \"select * from [dbo].[rt_t1100_DataReconMappings]\"\n",
    "    df1= pd.read_sql(Query, conn)\n",
    "\n",
    "    dimensions = df1[df1['Measure/Dimension']=='D']\n",
    "    measures = df1[df1['Measure/Dimension']=='M']\n",
    "\n",
    "    #CREATING FILTERS FOR TABLE 1\n",
    "    dimensions_1 = dimensions.loc[dimensions['Granularity']==1,'List of Columns']\n",
    "    measures_1 = measures['List of Columns']\n",
    "    #CREATING ISNULL(FILTER COLUMNS)\n",
    "    dimensions_filter = dimensions.copy()\n",
    "    dimensions_filter['List of Columns'] = \"isnull(\" + dimensions_filter['List of Columns'] + \",'')\"\n",
    "    filters_1 = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns','Filter']]\n",
    "    filters_1 = filters_1.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "\n",
    "    #CREATING FILTERS FOR TABLE 2\n",
    "    dimensions_2 = dimensions.loc[dimensions['Granularity']==1,'List of Columns1']\n",
    "    measures_2 = measures['List of Columns1']\n",
    "    #CREATING ISNULL(FILTER COLUMNS)\n",
    "    dimensions_filter['List of Columns1'] = \"isnull(\" + dimensions_filter['List of Columns1'] + \",'')\"\n",
    "    filters_2 = dimensions_filter.loc[dimensions_filter['Filter'].notnull(),['List of Columns1','Filter']]\n",
    "    filters_2 = filters_2.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "\n",
    "    #CREATING GROUP BY GRAINS FROM TABLE 1\n",
    "    Grains_1 = dimensions_1.str.cat(sep=',') \n",
    "    #CREATING GROUP BY GRAINS FROM TABLE 2\n",
    "    Grains_2 = dimensions_2.str.cat(sep=',') \n",
    "\n",
    "    \"\"\"\n",
    "    Distinct check. Get grains to check distinct on. Run Except Query.\n",
    "    \"\"\"\n",
    "\n",
    "    Distinct_Grains_1 = df1.loc[df1['Distinct Check']==1,'List of Columns'].str.cat(sep=',')\n",
    "    Distinct_Grains_2 = df1.loc[df1['Distinct Check']==1,'List of Columns1'].str.cat(sep=',')\n",
    "\n",
    "    Distinct_Check_Qry_1 = \"\"\"SELECT %s \n",
    "                            FROM  (Select a.*, b.Bu_code from dbo.[super actsubdb_miu_2020Q1_bookings] a\n",
    "                            left join sparta.dbo.rt_t0025_mas_le b\n",
    "                            on a.[MIU Carco] = b.[MIU Carco]) c \n",
    "                            WHERE %s \n",
    "                            Except\n",
    "                            SELECT %s \n",
    "                            FROM  dbo.[MIU_SISAD_ITC_202001] \n",
    "                            WHERE %s\"\"\" % (Distinct_Grains_1,filters_1,Distinct_Grains_2,filters_2)\n",
    "\n",
    "\n",
    "    Distinct_Check_Qry_2 = \"\"\"SELECT %s \n",
    "                            FROM  dbo.[MIU_SISAD_ITC_202001] \n",
    "                            WHERE %s\n",
    "                            EXCEPT\n",
    "                            SELECT %s \n",
    "                            FROM  (Select a.*, b.Bu_code from dbo.[super actsubdb_miu_2020Q1_bookings] a\n",
    "                            left join sparta.dbo.rt_t0025_mas_le b\n",
    "                            on a.[MIU Carco] = b.[MIU Carco]) c \n",
    "                            WHERE %s \"\"\" % (Distinct_Grains_2,filters_2,Distinct_Grains_1,filters_1)\n",
    "\n",
    "    Distinct_Check_Qry_1_Res= pd.read_sql(Distinct_Check_Qry_1, conn)\n",
    "    Distinct_Check_Qry_2_Res= pd.read_sql(Distinct_Check_Qry_2, conn1)\n",
    "    f_path = r'%s\\DistinctCheck.xlsx' %(save_file_path)\n",
    "    with pd.ExcelWriter(f_path) as writer:  \n",
    "        Distinct_Check_Qry_1_Res.to_excel(writer, sheet_name='SubDB vs SISAD')\n",
    "        Distinct_Check_Qry_2_Res.to_excel(writer, sheet_name='SISAD vs SubDB')\n",
    "    writer.save()\n",
    "    print(\"Distinct Check is complete.\") \n",
    "\n",
    "def granularCheck():\n",
    "    global str5\n",
    "    global str6 ,obu_names,file_level,d\n",
    "    conn = odbc.connect(connectionstr1)    \n",
    "    conn1 = odbc.connect(connectionstr2)  \n",
    "    # obu_names = \"''SP - ASB Art''\" \n",
    "    Start_Time = datetime.datetime.now()\n",
    "\n",
    "    Query = \"Update ct set ct.Filter = 'IN ( %s )' FROM dbo.[rt_t1100_DataReconMappings] ct WHERE SNo = '%s' and Type_of_Recon  = '%s' and isnull(type_of_measure,'') in ('',%s)\" % (obu_names,file_level,recon_type,measure_names)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(Query)\n",
    "    print(Query)\n",
    "\n",
    "    Query = \"select * from dbo.[rt_t1100_DataReconMappings] WHERE Type_of_Recon  = '%s' and isnull(type_of_measure,'') in ('',%s)\" %(recon_type,measure_names)\n",
    "    df1= pd.read_sql(Query, conn)\n",
    "\n",
    "    dimensions = df1[df1['Measure/Dimension']=='D']\n",
    "    measures = df1[df1['Measure/Dimension']=='M']\n",
    "\n",
    "\n",
    "    #CREATING FILTERS FOR TABLE 1\n",
    "    \n",
    "    dimensions_D = dimensions['List of Columns']\n",
    "    dimensions_1 = dimensions.loc[dimensions['Granularity']==1,'List of Columns']\n",
    "    measures_1 = measures['List of Columns']\n",
    "    \n",
    "    #CREATING ISNULL(FILTER COLUMNS)\n",
    "    dimensions_filter_T = dimensions.copy()\n",
    "    dimensions_filter_T['List of Columns'] = \"isnull(\" + dimensions_filter_T['List of Columns'] + \",'')\"\n",
    "    filters_1_T = dimensions_filter_T.loc[dimensions_filter_T['Filter'].notnull(),['List of Columns','Filter']]\n",
    "    filters_1_T = filters_1_T.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "\n",
    "    #CREATING FILTERS FOR TABLE 2\n",
    "    dimensions_D_2 = dimensions['List of Columns1']\n",
    "    dimensions_2 = dimensions.loc[dimensions['Granularity']==1,'List of Columns1']\n",
    "    measures_2 = measures['List of Columns1']\n",
    "\n",
    "    #CREATING ISNULL(FILTER COLUMNS)\n",
    "    dimensions_filter_T['List of Columns1'] = \"isnull(\" + dimensions_filter_T['List of Columns1'] + \",'')\"\n",
    "    filters_2_T = dimensions_filter_T.loc[dimensions_filter_T['Filter'].notnull(),['List of Columns1','Filter']]\n",
    "    filters_2_T = filters_2_T.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).str.cat(sep=' AND ')\n",
    "\n",
    "    #CREATING SELECT GRAINS FROM TABLE 1 for Total Check \n",
    "    Grains_Select1_D = \"upper(ltrim(rtrim(\" + dimensions_1 + \"))) \"  +  dimensions_1\n",
    "    Grains_Select1_D = Grains_Select1_D.str.cat(sep=',') \n",
    "\n",
    "    #CREATING SELECT GRAINS FROM TABLE 2\n",
    "    Grains_Select2_D = \"upper(ltrim(rtrim(\" + dimensions_2 + \"))) \"  + dimensions_1\n",
    "    Grains_Select2_D = Grains_Select2_D.str.cat(sep=',') \n",
    "\n",
    "\n",
    "    #CREATING GROUP BY GRAINS FROM TABLE 1\n",
    "    Grains_1_D = dimensions_1.str.cat(sep=',') \n",
    "\n",
    "    #CREATING GROUP BY GRAINS FROM TABLE 2\n",
    "    Grains_2_D = dimensions_2.str.cat(sep=',') \n",
    "\n",
    "    # measures_Premiums = measures[measures['type_of_measure']=='Premium']\n",
    "\n",
    "    #CREATING PREMIUM MEASURES \n",
    "    measures_1 = measures['List of Columns']\n",
    "    measures_2 = measures['List of Columns1']\n",
    "\n",
    "    #TABLE1\n",
    "    measures_grain_check_1 = \"Sum(\" + measures_1 + \") \" + measures_2\n",
    "    measures_grain_check_1 = measures_grain_check_1.str.cat(sep=',') \n",
    "\n",
    "    #TABLE2\n",
    "    measures_grain_check_2 = measures['sign_convention'] + \"*-1*Sum(\" + measures['List of Columns1'] + \")\" + \" \" + + measures_2\n",
    "    measures_grain_check_2 = measures_grain_check_2.str.cat(sep=',')\n",
    "    \n",
    "     #HAVING CONDITION\n",
    "    measures_grain_check_H = \" Sum(\" + measures_1 + \") <> 0 \" \n",
    "    measures_grain_check_H = measures_grain_check_H.str.cat(sep='OR') \n",
    "\n",
    "    measures_grain_check_H_2 = \" Sum(\" + measures_2 + \") <> 0 \" \n",
    "    measures_grain_check_H_2 = measures_grain_check_H_2.str.cat(sep='OR') \n",
    "\n",
    "    File_Name_Level_1 = dimensions.loc[dimensions['SNo']== file_level,'List of Columns']\n",
    "    File_Name_Level_2 = dimensions.loc[dimensions['SNo']== file_level,'List of Columns1']\n",
    "\n",
    "    print(\"Running Total Check....\")\n",
    "\n",
    "    GrainChecks_qry1 = \"\"\"SELECT 'Actuarialdb' source, %s ,\n",
    "            %s \n",
    "            FROM  %s c \n",
    "            WHERE %s \n",
    "            GROUP BY %s\n",
    "            HAVING (%s)\n",
    "            \"\"\"% (Grains_Select1_D, measures_grain_check_1,str5,filters_1_T, Grains_1_D,measures_grain_check_H)\n",
    "\n",
    "    GrainChecks_qry2 =\"\"\"\n",
    "            SELECT 'SISAD' source, %s ,\n",
    "                %s \n",
    "                FROM  %s\n",
    "                WHERE %s \n",
    "                GROUP BY %s\n",
    "                HAVING (%s)\n",
    "            \"\"\" % (Grains_Select2_D, measures_grain_check_2,str6,filters_2_T, Grains_2_D,measures_grain_check_H_2)\n",
    "\n",
    "    GrainChecks_qry_Res1 = pd.read_sql(GrainChecks_qry1, conn)\n",
    "    GrainChecks_qry_Res2 = pd.read_sql(GrainChecks_qry2, conn1)\n",
    "    GrainChecks_qry_Res =  pd.concat([GrainChecks_qry_Res1, GrainChecks_qry_Res2])\n",
    "\n",
    "\n",
    "    column1 = File_Name_Level_1.apply(lambda x: x[1:len(x)-1])\n",
    "    column2 = File_Name_Level_2.apply(lambda x: x[1:len(x)-1])\n",
    "\n",
    "    d= pd.Series((GrainChecks_qry_Res[File_Name_Level_1.apply(lambda x: x[1:len(x)-1])].iloc[:,0])).unique()\n",
    "\n",
    "    from pandas import ExcelWriter\n",
    "    OBU = ''.join(e for e in obu_names[1:len(obu_names)-1] if e.isalnum())\n",
    "\n",
    "    print(\"Running Grain level Checks....\")\n",
    "\n",
    "    type_of_measures = pd.Series(measures['type_of_measure']).unique()\n",
    "    for i in range(len(d)):\n",
    "        \n",
    "        FilePath = r\"%s\\Granular\\%s.xlsx\" %(save_file_path ,d[i]) \n",
    "        writer = pd.ExcelWriter(FilePath)\n",
    "        e=GrainChecks_qry_Res[column1] == d[i]\n",
    "        es = pd.Series(e.iloc[:,0])\n",
    "        data = GrainChecks_qry_Res[es]\n",
    "        data.to_excel(writer, sheet_name='Total')\n",
    "        for j in range(len(type_of_measures)):\n",
    "            comment = \"Running %s Grain level Checks for %s....\" %(type_of_measures[j],d[i])\n",
    "            print(comment)\n",
    "            measures_G = measures[measures['type_of_measure']==type_of_measures[j]]\n",
    "            measures_2 = measures_G['List of Columns1']\n",
    "            dimensions_1.reset_index(drop=True,inplace = True)\n",
    "          \n",
    "            for k in range(len(dimensions_1)):\n",
    "                individual_grain = pd.Series('source').append(pd.Series(dimensions_1[k][1:len(dimensions_1[k])-1]))\n",
    "                Individual_Check_columns = individual_grain.append(measures_2.apply(lambda x: x[1:len(x)-1]))           \n",
    "                GrainChecks_qry_Res_P = data[Individual_Check_columns]    \n",
    "                sheet_name = type_of_measures[j][0] + '-' + ''.join(e for e in dimensions_1[k][1:len(dimensions_1[k])-1] if e.isalnum())\n",
    "                GrainChecks_qry_Res_P= GrainChecks_qry_Res_P.fillna('dummy')\n",
    "                table = pd.pivot_table(GrainChecks_qry_Res_P,index= dimensions_1[k][1:len(dimensions_1[k])-1],values=measures_2.apply(lambda x: x[1:len(x)-1]),columns= [\"source\"],aggfunc=[np.sum], margins=True,fill_value=0).reset_index().replace('dummy',np.nan).set_index(dimensions_1[k][1:len(dimensions_1[k])-1])\n",
    "                table.to_excel(writer, sheet_name=sheet_name)\n",
    "        writer.save()\n",
    "    \n",
    "    End_Time = datetime.datetime.now()\n",
    "    print(\"Granular Check is complete in: \") \n",
    "    print(End_Time-Start_Time)\n",
    "\n",
    "def runAll():\n",
    "    granularCheck()\n",
    "    debugCheck()\n",
    "\n",
    "def rw_ui():\n",
    "    global df,recon_type\n",
    "    global call,app    \n",
    "    nb_row = df.shape[0]\n",
    "    nb_col = df.shape[1]\n",
    "    str1=''\n",
    "    for row in range (nb_row):\n",
    "            for col in range(nb_col):\n",
    "                df.iloc[row,col]=call.Table1.item(row, col).text()\n",
    "                # print(call.Table1.item(row, col).text())\n",
    "    \n",
    "    df.fillna('',inplace=True)\n",
    "    conn = odbc.connect('Driver={SQL Server};'\n",
    "                      'Server=actuarialdb;'\n",
    "                      'Database=actuarialdb;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "    del_q=\"DELETE from dbo.rt_t1100_DataReconMappings where Type_Of_recon ='\"+recon_type+\"'\"\n",
    "    conn.execute(del_q)\n",
    "    for row in range (nb_row):\n",
    "        str1=''\n",
    "        for col in range(nb_col):\n",
    "            # columns1=columns1.str.replace(\"[\",\"'\")\n",
    "            str1=str1+df.iloc[row,col].replace(\"'\",\"$\")+\"','\"\n",
    "            \n",
    "        string=str1[:-2] \n",
    "        string=string.replace(\"'-999'\",\"NULL\")\n",
    "        string=string.replace(\"''\",\"NULL\")\n",
    "        cursor=conn.cursor()\n",
    "        qry=\"INSERT INTO dbo.rt_t1100_DataReconMappings([SNo], [List of Columns],[List of Columns1],[Measure/Dimension],[Filter],[NULL Check],[Distinct Check],[Granularity],[Priority],[Length Check],[Datatype Check],[sign_convention],[type_of_measure],[type_of_recon]) \"\n",
    "        qry=qry+\" values('\"+string+\",'\"+recon_type+\"')\" \n",
    "        # print(qry)   \n",
    "        cursor.execute(qry)\n",
    "        cursor.execute(\"update dbo.rt_t1100_DataReconMappings set filter =replace(filter,'$','''') where Type_Of_recon = '\" +recon_type + \"'\")\n",
    "        cursor.commit()\n",
    "        cursor.close()\n",
    "    app.exec_()\n",
    "    # app=QtWidgets.QApplication([])\n",
    "    call=uic.loadUi(\"rw.ui\")\n",
    "    call.show()\n",
    "    app.exec()\n",
    "    call.pushButton2.clicked.connect(clearcombo)\n",
    "    call.saveserver.clicked.connect(getdb)\n",
    "    call.savedb.clicked.connect(gettables)\n",
    "    call.savetable.clicked.connect(getall)\n",
    "    call.pushButton.clicked.connect(runAll)\n",
    "    call.addfilter.clicked.connect(filteraddition)\n",
    "    call.addmeasure.clicked.connect(filteradditionmeasure)\n",
    "    call.confirmfilter.clicked.connect(filterfinal)\n",
    "    call.confirmmeasure.clicked.connect(filterfinalmeasure)\n",
    "    call.confirmfilter0.clicked.connect(setfiltercolumn)\n",
    "    call.editmappings.clicked.connect(table_ui)\n",
    "    call.woediting.clicked.connect(withoutediting)\n",
    "\n",
    "def addrow():\n",
    "    data=''\n",
    "    global df\n",
    "    call.Table1.setRowCount(df.shape[0]+1)\n",
    "    # dff=df.copy()\n",
    "    df.loc[df.iloc[-1].name + 1,:] = np.nan\n",
    "    df.fillna('',inplace=True)\n",
    "    for row in range (df.shape[0]-1, df.shape[0]):\n",
    "        for col in range (0, df.shape[1]):\n",
    "            data = str(df.iat[row, col] )\n",
    "            call.Table1.setItem(row, col, item(str(data)))\n",
    "\n",
    "    # df.loc[df.iloc[-1].name + 1,:] = np.nan\n",
    "\n",
    "\n",
    "call.pushButton2.clicked.connect(clearcombo)\n",
    "call.saveserver.clicked.connect(getdb)\n",
    "call.savedb.clicked.connect(gettables)\n",
    "call.savetable.clicked.connect(getall)\n",
    "call.pushButton.clicked.connect(runAll)\n",
    "call.addfilter.clicked.connect(filteraddition)\n",
    "call.addmeasure.clicked.connect(filteradditionmeasure)\n",
    "call.confirmfilter.clicked.connect(filterfinal)\n",
    "call.confirmmeasure.clicked.connect(filterfinalmeasure)\n",
    "call.confirmfilter0.clicked.connect(setfiltercolumn)\n",
    "call.editmappings.clicked.connect(table_ui)\n",
    "call.woediting.clicked.connect(withoutediting)\n",
    "\n",
    "call.show()\n",
    "app.exec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
